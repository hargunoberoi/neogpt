# Model Configuration
model:
  block_size: 64
  batch_size: 128
  n_embd: 256
  n_head: 8
  n_layer: 8
  dropout: 0.2
  vocab_size: 512

# Training Configuration
training:
  max_iters: 3000
  learning_rate: 0.0003 # yaml treats 3e-4 as a string
  eval_iters: 300
  eval_interval: 100
  update_threshold: 0.1
# Generation Configuration
generation:
  max_new_tokens: 200


